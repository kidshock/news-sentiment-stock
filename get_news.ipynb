{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from 2024-10-07 to 2024-10-14\n",
      "Fetching news from 2024-10-15 to 2024-10-22\n",
      "Fetching news from 2024-10-23 to 2024-10-30\n",
      "Fetching news from 2024-10-31 to 2024-11-06\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "API_KEY = 'MY API KEY'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {API_KEY}'\n",
    "}\n",
    "\n",
    "# Function to get news from Polygon.io API (only title, content, publisher, and published date)\n",
    "def get_news(ticker, start_date, end_date):\n",
    "    base_url = 'https://api.polygon.io/v2/reference/news'\n",
    "    url = f'{base_url}?ticker={ticker}&published_utc.gte={start_date}&published_utc.lte={end_date}&order=desc&limit=100'\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['results']\n",
    "    else:\n",
    "        print(f'Error {response.status_code}: {response.json().get(\"error\", \"Unknown error\")}')\n",
    "        return []\n",
    "\n",
    "# Function to remove duplicate news articles based on title\n",
    "def remove_duplicates(news_data):\n",
    "    seen_titles = set()\n",
    "    unique_news = []\n",
    "    \n",
    "    for news in news_data:\n",
    "        title = news.get('title', '')\n",
    "        if title not in seen_titles:\n",
    "            seen_titles.add(title)\n",
    "            unique_news.append(news)\n",
    "    \n",
    "    return unique_news\n",
    "\n",
    "# Function to fetch news over a time period with rate limit handling\n",
    "def fetch_news(ticker, num_days):\n",
    "    news_data = []\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=num_days)\n",
    "    api_calls = 0\n",
    "    last_api_call_time = time.time()\n",
    "\n",
    "    while start_date < end_date:\n",
    "        current_end = min(start_date + timedelta(days=7), end_date)\n",
    "        print(f\"Fetching news from {start_date.date()} to {current_end.date()}\")\n",
    "\n",
    "        # Check if we need to wait before making the next API call\n",
    "        if api_calls >= 5:\n",
    "            time_since_last_call = time.time() - last_api_call_time\n",
    "            if time_since_last_call < 60:\n",
    "                wait_time = 60 - time_since_last_call\n",
    "                print(f\"Reached API limit, waiting {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            api_calls = 0\n",
    "            last_api_call_time = time.time()\n",
    "\n",
    "        news_batch = get_news(ticker, start_date.strftime('%Y-%m-%d'), current_end.strftime('%Y-%m-%d'))\n",
    "        news_data.extend(news_batch)\n",
    "        \n",
    "        api_calls += 1\n",
    "        start_date = current_end + timedelta(days=1)\n",
    "\n",
    "    return remove_duplicates(news_data)\n",
    "\n",
    "# Main function to execute\n",
    "def main():\n",
    "    ticker = 'NVDA'  # For example, NVDA\n",
    "    num_days =  30  # Two years\n",
    "    news_data = fetch_news(ticker, num_days)\n",
    "\n",
    "    # Extract required fields (title, content, publisher, and published date)\n",
    "    cleaned_data = [\n",
    "        {\n",
    "            'title': news.get('title', ''),\n",
    "            'content': news.get('description', ''),  # Description as content\n",
    "            'publisher': news.get('publisher', {}).get('name', ''),  # Publisher name\n",
    "            'published_date': news.get('published_utc', '')  # Published date\n",
    "        }\n",
    "        for news in news_data\n",
    "    ]\n",
    "\n",
    "    # Convert to DataFrame and save to CSV\n",
    "    df = pd.DataFrame(cleaned_data)\n",
    "    df.to_csv('unseenNews.csv', index=False, encoding='utf-8')\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
